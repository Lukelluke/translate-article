#### TACOTRON

Abstract





#### Introduction

- 现代的TTS系统是复杂的，例如，统计参数TTS通常具有
  1. **提取各种语言特征的文本前端**，
  2. **持续时间模型**，
  3. **声学特征预测模型**，                                             acoustic model 
  4. **基于复杂信号处理的声码器**                                  vocoder

- **缺点**：

  这些组件基于广泛的领域专门知识，设计起来很费劲。

  它们也是独立训练的，因此来自每个部件的误差可能会复合。

  因此，现代TTS设计的复杂性导致了在构建新系统时所付出的巨大工程努力。

- **整体的端到端系统优点**

  1. 首先，这样一个系统减轻了对复杂的特性工程的需求，这可能包括探索中的脆弱的设计选择。
  2. 其次，它更容易在各种属性(如说话人或语言)或高级特性(如情感)上提供丰富的条件。这是因为相关条件可以发生在模型的最开始，而不仅仅是在某些组件上。同样，适应新数据也可能更容易。
  3. 最后，单个模型可能比多阶段模型更有鲁棒性，而在多阶段模型中，每个组件的误差可能会复合。

  **这些优点意味着，端到端的模型可以让我们在真实世界中发现的大量丰富的、有表现力的、但通常是嘈杂的数据上进行培训。**

- TTS是一个大规模的逆问题：一个高度压缩的源（文本）被“被解压缩的”转换为音频。

  1. 由于相同的文本可以对应于不同的发音或说话方式，所以对于端到端模型来说，这是一个特别困难的学习任务：对于给定的输入，它必须在信号电平上处理较大的变化。
  2. 此外，与端到端语音识别或机器翻译不同，TTS输出是连续的，并且输出序列通常比输入的时间长得多。

  这些特点导致预测错误迅速累积。

- 在本文中，我们提出了一种基于序列到序列(Seq2seq)(Sutskever等人，2014年)和注意力范式的端到端生成TTS模型(Bahdanau等人，2014年)。

  1. 我们的模型以字符作为输入，输出原始谱图，使用多种技术来提高一个普通的seq2seq模型的性能。
  2. 给定的<文本、音频>对，Tacotron可以通过随机初始化从头开始完全训练。它不需要音素水平对齐，因此它可以很容易地使用大量的声音数据与转录。
  3. **用简单的波形合成技术**，塔科创在美国英语标准集上的平均意见评分(MOS)为3.82分，在自然性方面优于生产参数系统。

#### Related work

- WaveNET(van den Oord等人，2016)是一种强大的音频生成模型。它适用于TTS，
  1. 由于它的（sample-level autoregressive nature）样本级自回归特性，它是缓慢的.
  2. 它还需要从现有TTS前端调节语言特征，因此，它并不是端到端：它只取代了声码器和声学模型。
- 另一个最近开发的神经模型是DeepoVoice(Arik等，2017)，其通过相应的神经网络来代替典型TTS管道中的每个组件。
  1. 但是，每个组件都经过独立培训，将系统更改为以端到端的方式进行培训并不是一件容易的事。
- 据我们所知，王等。(2016)是最早使用seq2seq和注意机制进行端到端tts的作品。
  1. 然而，它需要一个预先训练的隐马尔可夫模型(HMM)对齐器来帮助seq2seq模型学习对齐。很难说seq2seq本身学习了多少对齐。
  2. 使用一些技巧来训练模型，作者注意到这会伤害韵律。
  3. 对声码器参数进行了预测。因此，需要声码器。
  4. 此外，该模型对音素输入进行了训练，并且实验结果似乎有些有限。
- Char2WAV(SoTelo等人，2017)是一个独立开发的端到端模型，可以对字符进行训练。
  1. 然而，在使用SampleRNN神经声码器之前，Char2Wav仍然预测声码器参，而Tacotron可直接预测原始光谱图
  2. 另外，他们的seq2seq和sampleRnn模型需要分别进行预训练，但是我们的模型可以从零开始训练
  3. 最后，我们对普通的seq2seq范式做了一些关键的修改。如后所示，普通的seq2seq模型不适用于字符级输入。

#### Model Architecture

Taoctron的骨架是具有attention机制的Seq2Seq模型。图1描述了模型，

 	1. 包括一个编码器、
 	2. 一个基于注意力的解码器
 	3. 一个后处理网络.

在高层次上，我们的模型

1. 以字符为输入，
2. 生成频谱图框架，
3. 然后将其转换为波形。

- **CBHG模块**

- **编码器模块**

- **解码器模块**