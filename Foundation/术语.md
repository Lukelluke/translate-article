#### 专有名词

- 英语名词
  - data set                                          
  - instance
  - sample
  - attribute
  - feature
  - attribute value
  - attribute space
  - exposure bias       RNN网络中的一种偏差
  - $:=$ 表示更新参数
  -  softmax ： 将逻辑回归的输出转换为一组概率，且这组概率的和为1（概率分布）
  - 交叉熵损失函数：搭配softmax使用的损失函数，常用于分类问题
  - nnl：神经网络loss
  - 

#### 个人总结

1. 深度学习

   一个事物有b个特征，即可以在$\{x_1 , x_2 , \cdots , x_b\}$ 维的空间中表示为一个点，那么如果训练出来的预测的 $\hat{y}$ 与实际情况的 $y$ 一致或者差不多， 那么他们在该空间中距离也是相近的，所以对于事物的分类什么的，大家就显式地使用点之间的距离来进行判别。

2. 非线性模型是怎么得到的

   在线性模型的基础上通过引入   **层级结构**   或    **高维映射**  得到的

3. 正则项

   - 现实中 $X^TX$ 往往不是满秩矩阵，即变量特别多（特征多），规律少（特征之间的条件少），即 $X$ 列数多于行数，导致有多个解

   - 多个解都可以使得均方误差最小化，选择哪一个解作为输出就很关键

   - 正则化就是属于学习算法的归纳偏好。

4. 归纳偏好

   - 训练集数据和实际使用比起来总是小的，（将训练数据看作一个个高维空间的点），训练时，产生的模型（空间点的分布函数）可以有多个，并且这多个模型覆盖训练数据的情况都是一样的，比如：
     $$
      y=x \\y=x^2
     $$

   - 这两个函数对于只含有一个点$（1,1）$的训练集表现出一样的训练效果，但是他两函数实际差距很大，所以，我们需要其他办法来选择模型，选择多个模型中的一个。

5.  举例 ( log-linear regression )  通过线性模型转换为非线性


$$
ln y = w^Tx+b\\
y=e^{w^Tx+b}\\
$$

​	更一般的
$$
y=g^{-1}(w^Tx+b)\\
$$
​	对数线性回归是$g(\cdot) = ln(\cdot)$ 的一个特例。要求 $g(\cdot)$ 单调可微。

6. 使用线性模型做分类

   - 使用单位跃迁函数将实数值转换为0/1值，
     $$
     y = 0 \quad z<0 \\
     y = 0.5 \quad z=0 \\
     y = 1 \quad z>0 \\
     $$

   - 由于单位跃迁函数不连续，所以不能作为 $g^-{(\cdot)}$ ，所以我们使用一个替代函数，即为sigmoid 函数。 
     $$
     y=\frac{1}{1+e^{-z}} \\
     $$

     $$
     y=\frac{1}{1+e^{-(w^Tx+b)}}
     $$

     $$
     \frac{1-y}{y}=e^{-{(w^Tx+b)}}
     $$

     $$
     ln\frac{1-y}{y}=-(w^Tx+b)
     $$

     $$
     ln\frac{y}{1-y}=w^Tx+b
     $$

   - 将 y 和 1-y 看作正例和反例，可以通过极大似然法来计算

7. 极大似然法

   - 研究背景（例子）：有两个外形完全相同的箱子，甲箱中有99个白球，一个黑球；乙箱中有99只黑球，1只白球。一次试验取出一个球，结果取出的是黑球。问，黑球是从哪个箱子里面取出来的概率最高？

     - 最大可能是从乙箱中取出来的，这种想法就是最大似然。

   - 最大似然定理（maximum-likelihood）: 利用已知的样本结果，反推最有可能导致这种结果的参数值

   - 数学表示

     - [ ] 样本集 $D = \{x_1 , x_2 , \cdots, x_N\}$ 

     - [ ] 似然函数 $p(D|\theta)$ 称为相对于 $\{x_1  ,x_2,x_3,\cdots , x_N\}$ 的 $\theta$ 的似然函数 
       $$
       l(\theta) = p(D|\theta)=p(x_1 , x_2 ,\cdots ,x_N | \theta) = \prod_{i=1}^N p(x_i|\theta)
       $$

     - [ ] 如果 $\hat{\theta}$ 是参数空间中能使似然函数 $l(\theta)$ 最大的 $\theta$ 值，则 $\hat{\theta}$ 应该就是“最有可能”的参数值，那么，$\hat{\theta}$ 就是 $\theta$  的极大似然估计量。它是样本集的函数，记作:
       $$
       \hat{\theta}=d(x_1,x_2,\cdots ,x_N) = d(D)
       $$

   - 如何求解

     - [ ] 求解目标：求使得出现该样本的概率最大的 $\theta$ 值
       $$
       \hat{\theta} = arg \max\limits_{\theta}l(\theta) = arg \max\limits_{\theta}\prod_{i=1}^Np(x_i|\theta)
       $$
       实际中为了便于分析，定义了对数似然函数
       $$
       H(\theta) = \ln l(\theta)
       $$
       则：
       $$
       \hat{\theta} = arg\max\limits_{\theta}H(\theta) = arg\max\limits_{\theta}\sum_{i=1}^N \ln p(x_i|\theta)
       $$

     - [ ] 未知数只有一个( $\theta $ 为标量) [地址](<https://blog.csdn.net/qq_39355550/article/details/81809467>)

     - [ ] 未知数有多个（ $\theta$ 为向量） 

8.  推导反向传播

    [用Python实现BP神经网络（附代码）](https://zhuanlan.zhihu.com/p/32458271)

    [用Python从头实现一个神经网络 - 陈钢的文章 - 知乎](https://zhuanlan.zhihu.com/p/58964140)

9.  推导正则项

10.  推导dropout

11.  归一化输入

12.  优化算法

13.  batch归一化

     对于任何一个隐藏层而言，我们归一化a值或z值。（$ a= \sigma(z) $）

14. batch归一化如何拟合入神经网络

15. [《书籍代码》](<https://github.com/MichalDanielDobrzanski/DeepLearningPython35>)

16. [机器学习的MLE和MAP：最大似然估计和最大后验估计](https://zhuanlan.zhihu.com/p/32480810)

17. [SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS](https://zhuanlan.zhihu.com/p/22649457)

18. [李宏毅老师的笔记](<https://datawhalechina.github.io/leeml-notes/#/chapter14/chapter14?id=%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad>)

19. [南瓜书](<https://datawhalechina.github.io/pumpkin-book/#/>)

20. [L1与L2损失函数和正则化的区别](https://www.cnblogs.com/jclian91/p/9824310.html)

21. [阅读编辑器](<https://www.zybuluo.com/mdeditor>)

22. 

23. ·

24. ·

25. ···

- 1
- 1
- 1
- 1
- 1
- 1
- 1
- 1
- 1
- 1
- 1
- 11
- 1
- 1
- 1
- 1
- 1
- 
- 

